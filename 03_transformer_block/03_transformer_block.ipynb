{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9XwkZuo0yd46"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, d_model, eps=1e-5):\n",
        "    super().__init__()\n",
        "\n",
        "    self.gamma = nn.Parameter(torch.ones(d_model))\n",
        "\n",
        "    self.beta = nn.Parameter(torch.zeros(d_model))\n",
        "\n",
        "    self.eps = eps\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    mean = x.mean(dim=-1, keepdim=True)\n",
        "\n",
        "    var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "\n",
        "    x_hat = (x - mean) / torch.sqrt(var + self.eps)\n",
        "\n",
        "    return self.gamma * x_hat + self.beta\n"
      ],
      "metadata": {
        "id": "oT-m5V440lhh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.fc1 = nn.Linear(d_model, d_ff)\n",
        "    self.fc2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    x = self.fc1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "bTBqJxtbGb63"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_model, num_heads, dropout=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    assert d_model % num_heads == 0\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = d_model // num_heads\n",
        "\n",
        "    self.W_q = nn.Linear(d_model, d_model)\n",
        "    self.W_k = nn.Linear(d_model, d_model)\n",
        "    self.W_v = nn.Linear(d_model, d_model)\n",
        "\n",
        "    self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def split_heads(self, x):\n",
        "\n",
        "    batch_size, seq_len, _ = x.shape\n",
        "\n",
        "    x = x.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "\n",
        "    return x.transpose(1,2)\n",
        "\n",
        "  def forward(self, x, mask=None):\n",
        "\n",
        "    Q = self.W_q(x)\n",
        "    K = self.W_k(x)\n",
        "    V = self.W_v(x)\n",
        "\n",
        "    Q = self.split_heads(Q)\n",
        "    K = self.split_heads(K)\n",
        "    V = self.split_heads(V)\n",
        "\n",
        "    scores = torch.matmul(Q, K.transpose(-2, -1))\n",
        "    scores = scores / math.sqrt(self.head_dim)\n",
        "\n",
        "    if mask is not None:\n",
        "      scores = scores.masked_fill(mask==0, -1e9)\n",
        "\n",
        "    attn_weights = torch.softmax(scores, dim=-1)\n",
        "    attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "    attn_output = torch.matmul(attn_weights, V)\n",
        "\n",
        "    batch_size = x.shape[0]\n",
        "    attn_output = attn_output.transpose(1, 2)\n",
        "    attn_output = attn_output.contiguous().view(\n",
        "        batch_size, -1, self.d_model\n",
        "    )\n",
        "\n",
        "    return self.W_o(attn_output)"
      ],
      "metadata": {
        "id": "Ze11TAa9IHxM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "    self.ffn = FeedForward(d_model, d_ff, dropout)\n",
        "\n",
        "    self.norm1 = LayerNorm(d_model)\n",
        "    self.norm2 =LayerNorm(d_model)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x, mask=None):\n",
        "    attn_out = self.attention(x, mask)\n",
        "\n",
        "    x = self.norm1(x + self.dropout(attn_out))\n",
        "\n",
        "    ffn_out = self.ffn(x)\n",
        "\n",
        "    x = self.norm2(x + self.dropout(ffn_out))\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "rSNbHChHMUqR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 2\n",
        "seq_len = 5\n",
        "d_model = 32\n",
        "\n",
        "x = torch.randn(batch_size, seq_len, d_model)\n",
        "\n",
        "transformer = TransformerBlock(\n",
        "    d_model = 32,\n",
        "    num_heads = 4,\n",
        "    d_ff = 128\n",
        ")\n",
        "\n",
        "out = transformer(x)\n",
        "\n",
        "print(f\"shape: {out.shape}\")\n",
        "print(\"*\"*100)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiKtGwhUNJZA",
        "outputId": "d88c5efd-2976-4b58-d03b-1d04939b2b29"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: torch.Size([2, 5, 32])\n",
            "****************************************************************************************************\n",
            "tensor([[[ 1.0534, -0.6624, -1.4092, -0.4917,  0.5500,  0.0970,  0.1812,\n",
            "           0.4793,  0.6594, -0.1894, -1.3995,  0.7596,  1.0723, -0.6384,\n",
            "           0.9827, -0.5358,  2.2224, -0.3691,  0.0763, -1.7024, -1.1696,\n",
            "           1.1941, -0.3627,  0.1955, -0.3510,  1.8046, -0.8134,  1.0535,\n",
            "           0.1997,  0.4967, -2.1996, -0.7836],\n",
            "         [-0.3104,  0.2464, -0.4946, -1.2386, -1.6280,  1.0460, -0.6358,\n",
            "           1.7946, -1.0773,  0.4617, -0.1341, -2.0943,  1.0217,  0.1462,\n",
            "           0.9793, -0.2250,  0.2189, -1.6106,  0.5425, -1.3726, -0.1398,\n",
            "           1.1157,  0.3787,  0.1484,  0.5612,  0.7415, -0.2470, -0.1951,\n",
            "          -1.3836,  0.0124,  1.5640,  1.8076],\n",
            "         [ 2.3854, -1.8561, -0.8405, -0.2704,  0.9073,  0.0400,  1.4762,\n",
            "          -1.4033,  0.0074,  0.0974,  0.1561,  0.4720,  0.8220,  1.0027,\n",
            "          -0.8401, -1.3973,  0.3289, -0.3035,  1.8135, -1.6126,  0.3121,\n",
            "           0.0244, -0.8882, -0.6844,  0.1212,  0.4618, -0.0355, -1.0244,\n",
            "           0.9855, -1.1208, -0.3403,  1.2037],\n",
            "         [ 1.7744,  0.5712, -0.6032, -0.7200,  0.5672, -0.0269,  0.4502,\n",
            "           1.0436, -0.1394, -0.2290, -0.7883,  1.1664,  0.1407,  1.2869,\n",
            "          -0.0877,  0.2053,  1.6793, -1.7094, -0.1461, -1.6897, -0.2069,\n",
            "           0.9854,  0.8095,  1.5981, -1.7410,  0.8850, -1.1811, -0.5003,\n",
            "          -0.6984, -1.5606, -0.1316, -1.0034],\n",
            "         [-0.3289,  1.3481, -1.2294,  1.5068, -0.1812,  1.0942, -0.0316,\n",
            "          -0.9956, -0.1014, -0.4915,  0.1023, -1.4487,  0.3229,  0.1123,\n",
            "          -0.4137, -0.3040,  0.0509,  0.0824,  0.3355, -1.1738, -0.6619,\n",
            "          -1.5742,  0.1350,  0.9907,  0.3744,  2.8916,  0.8644, -0.9278,\n",
            "           0.5869,  1.0578, -2.0814,  0.0892]],\n",
            "\n",
            "        [[ 0.6181,  0.7092, -0.6674,  0.1858,  0.0839,  0.4497, -0.4844,\n",
            "           0.0199, -1.4903, -0.0824, -1.3193, -1.0479,  0.0029, -0.6034,\n",
            "          -1.2637, -0.6051, -0.7211, -0.4473,  0.9131, -0.0988,  0.9591,\n",
            "          -1.2801, -0.4864,  1.6775,  1.8239,  1.2598,  0.3126, -2.3030,\n",
            "           0.3403,  0.9858,  0.5912,  1.9678],\n",
            "         [ 1.0913, -0.6655, -0.6963, -0.0033, -0.0976,  1.4204,  0.0857,\n",
            "          -0.4119, -0.6387, -0.7952, -2.0352, -1.1356, -1.1163, -0.0873,\n",
            "          -0.0462,  0.0529,  1.7439,  1.9753,  1.0616,  0.3894, -0.9949,\n",
            "           2.3754, -1.0169, -0.0272, -0.6198, -0.5895, -0.2973,  0.8596,\n",
            "           0.5848,  1.0513, -0.5541, -0.8627],\n",
            "         [ 1.1894, -0.2369,  1.7838,  0.5700,  1.2006,  0.4203, -1.4325,\n",
            "           0.0400, -0.3339, -0.3630,  0.3438, -0.9077,  1.6447, -0.5502,\n",
            "          -0.5949,  1.0732,  0.8010, -1.0538,  1.7415, -0.9517,  0.2697,\n",
            "          -1.4812,  0.0563, -0.8279, -0.8670,  1.1611, -1.6046, -1.2473,\n",
            "           0.9698, -0.1726,  0.5745, -1.2145],\n",
            "         [ 0.6636, -0.5458, -1.6509, -0.0242,  0.8112,  1.3903,  0.3724,\n",
            "          -0.8346,  0.0439, -0.5667, -0.6145, -0.4835,  1.2430, -2.1214,\n",
            "           0.5919,  0.3160,  0.3707, -0.7237,  1.4748, -0.6703, -1.5840,\n",
            "          -0.4851, -0.4840,  0.8045, -1.0794,  0.8394,  0.8059,  0.4504,\n",
            "          -0.7276,  1.2097, -1.0275,  2.2354],\n",
            "         [ 0.1872, -1.4092,  1.3745, -0.0965,  0.1086, -0.0429, -1.0815,\n",
            "           1.1838, -0.6177, -1.5267, -0.1935, -1.6426, -0.3894, -0.1033,\n",
            "          -2.1172, -0.0281,  0.8802, -1.2327,  0.8799,  0.8540,  1.3812,\n",
            "          -0.6626, -0.9711,  2.1884,  0.3722, -0.4693,  1.4950,  0.1426,\n",
            "           0.0543,  0.1588,  0.7232,  0.6001]]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    }
  ]
}