{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Multi-Head Attention"
      ],
      "metadata": {
        "id": "B3I63TRPzkmD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import math"
      ],
      "metadata": {
        "id": "FDGToJ0hzjy4"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# random input with batch=1, tokens 4 with embedding vector of 8\n",
        "x = torch.randn(1,4,8)"
      ],
      "metadata": {
        "id": "Cv2_v0SEztzx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 8\n",
        "num_heads = 2\n",
        "head_dim = embed_dim // num_heads\n",
        "head_dim # from 1 (1,4,8) now we have two heads with dim = (1,4,2,4) - 2 head with 4d vector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMdtw4k8z4mY",
        "outputId": "686f7c91-6daf-497d-9b91-72a0d7efaaef"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# weight matrices for Q,K,V and output projection\n",
        "W_q = torch.randn(embed_dim,embed_dim)\n",
        "W_k = torch.randn(embed_dim,embed_dim)\n",
        "W_v = torch.randn(embed_dim,embed_dim)\n",
        "W_o = torch.randn(embed_dim,embed_dim)"
      ],
      "metadata": {
        "id": "J6uXUoU30k_9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Computing Q, K and V\n",
        "Q = x @ W_q\n",
        "K = x @ W_q\n",
        "V = x @ W_v\n",
        "Q.shape, K.shape, V.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwjcA8TT07mP",
        "outputId": "66df520e-34e4-4941-94cd-3cabce86b334"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 4, 8]), torch.Size([1, 4, 8]), torch.Size([1, 4, 8]))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_heads(t):\n",
        "  return t.view(1,4,num_heads,head_dim).transpose(1,2) # first (1,4,8) -> (1,4,2,4) -> (1,2,4,4)"
      ],
      "metadata": {
        "id": "capB9Tmg1Kq2"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Qh = split_heads(Q)\n",
        "Kh = split_heads(K)\n",
        "Vh = split_heads(V)\n",
        "Qh.shape, Kh.shape, Vh.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70KDvjqt2NeW",
        "outputId": "f11a308d-ae0b-4510-dbf3-a2f7dd4b2e25"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 2, 4, 4]), torch.Size([1, 2, 4, 4]), torch.Size([1, 2, 4, 4]))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Compute attention per heads\n",
        "scores = Qh @ Kh.transpose(-2,-1)\n",
        "\n",
        "#  Scaling\n",
        "scores = scores / math.sqrt(head_dim)\n",
        "\n",
        "# attention weights\n",
        "attn = F.softmax(scores, dim=-1)\n",
        "\n",
        "head_output = attn @ Vh"
      ],
      "metadata": {
        "id": "ZrKjgicO2csB"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## concatenate heads back to get original shape\n",
        "concate = head_output.transpose(1,2).reshape(1,4,embed_dim)"
      ],
      "metadata": {
        "id": "2Bx5vE-W3J7l"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('shape of head output: ',head_output.shape)\n",
        "print('concatenate result: ',concate.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gh36fzbO3tgA",
        "outputId": "a3343161-7ef8-455d-fda2-fac4b4c3adfa"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of head output:  torch.Size([1, 2, 4, 4])\n",
            "concatenate result:  torch.Size([1, 4, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('input\\n',x)\n",
        "print('multihead attention output\\n',concate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ep7NdQ9O32Cq",
        "outputId": "8ce33573-8d1a-4789-aef4-499ef7431d25"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input\n",
            " tensor([[[ 0.4994, -1.6186, -1.7349,  1.4511,  0.3490, -2.0378,  1.6342,\n",
            "          -0.2907],\n",
            "         [-0.0035, -0.6745, -1.4670, -2.0123, -0.5813,  0.1806,  0.1866,\n",
            "           0.1056],\n",
            "         [ 0.7883,  0.1376,  0.9274, -1.1609, -3.1396, -0.4939, -1.9834,\n",
            "          -0.5342],\n",
            "         [ 0.2885, -0.0091, -1.0703, -0.8698,  0.0283, -0.6103, -0.1859,\n",
            "          -0.9951]]])\n",
            "multihead attention output\n",
            " tensor([[[ 0.9287,  0.6518, -4.3290, -2.4623,  2.7119,  6.4714, -2.5216,\n",
            "          -5.8881],\n",
            "         [ 0.9205,  0.6397, -4.2778, -2.4562,  4.0769, -0.3710,  1.6185,\n",
            "          -0.1193],\n",
            "         [ 0.7359, -4.3003,  2.2744,  2.4425,  0.4189, -1.2835, -0.9693,\n",
            "           2.7203],\n",
            "         [ 0.7298,  0.2853, -3.3072, -2.2036,  0.9025, -0.0401, -1.1982,\n",
            "           1.2232]]])\n"
          ]
        }
      ]
    }
  ]
}