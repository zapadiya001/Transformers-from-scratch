{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjAOOS_FhpS-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Causal Self-Attention"
      ],
      "metadata": {
        "id": "j011IOPiiEh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super().__init__()\n",
        "    assert d_model % num_heads == 0\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = d_model // num_heads\n",
        "\n",
        "    self.W_q = nn.Linear(d_model, d_model)\n",
        "    self.W_k = nn.Linear(d_model, d_model)\n",
        "    self.W_v = nn.Linear(d_model, d_model)\n",
        "    self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    self.attn_dropout = nn.Dropout(0.1)\n",
        "    self.resid_dropout = nn.Dropout(0.1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, D = x.shape\n",
        "\n",
        "    Q = self.W_q(x)\n",
        "    K = self.W_k(x)\n",
        "    V = self.W_v(x)\n",
        "\n",
        "    Q = Q.view(B, T, self.num_heads, self.head_dim).transpose(1,2)\n",
        "    K = K.view(B, T, self.num_heads, self.head_dim).transpose(1,2)\n",
        "    V = V.view(B, T, self.num_heads, self.head_dim).transpose(1,2)\n",
        "\n",
        "    scores = Q @ K.transpose(-2, -1) / math.sqrt(self.head_dim)\n",
        "\n",
        "    causal_mask = torch.triu(torch.ones(T,T), diagonal=1).to(x.device)\n",
        "    scores = scores.masked_fill(causal_mask == 1, float('-inf'))\n",
        "\n",
        "    attn = F.softmax(scores, dim=-1)\n",
        "    attn = self.attn_dropout(attn)\n",
        "\n",
        "    out = attn @ V\n",
        "    out = out.transpose(1, 2).contiguous().view(B, T, D)\n",
        "\n",
        "    return self.resid_dropout(self.W_o(out))"
      ],
      "metadata": {
        "id": "Okr-R-Qnh4tz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Block (GPT - style)"
      ],
      "metadata": {
        "id": "K7TmFDSblHGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model, num_heads, ff_hidden):\n",
        "    super().__init__()\n",
        "    self.attn = CausalSelfAttention(d_model, num_heads)\n",
        "    self.norm1 = nn.LayerNorm(d_model)\n",
        "    self.ff = nn.Sequential(\n",
        "        nn.Linear(d_model, ff_hidden),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(ff_hidden, d_model),\n",
        "        nn.Dropout(0.1)\n",
        "    )\n",
        "    self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.attn(self.norm1(x))\n",
        "    x = x + self.ff(self.norm2(x))\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "OypOEoKSlEqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tiny GPT Model"
      ],
      "metadata": {
        "id": "ILJLp_gQm8XS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TinyGPT(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size, d_model, num_heads, ff_hidden, num_layers, block_size):\n",
        "    super().__init__()\n",
        "    self.token_emb = nn.Embedding(vocab_size, d_model)\n",
        "    self.pos_emb = nn.Embedding(block_size, d_model)\n",
        "\n",
        "    self.blockes = nn.ModuleList([\n",
        "        TransformerBlock(d_model, num_heads, ff_hidden)\n",
        "        for _ in range(num_layers)\n",
        "    ])\n",
        "\n",
        "    self.ln_f = nn.LayerNorm(d_model)\n",
        "    self.head = nn.Linear(d_model, vocab_size)\n",
        "    self.head.weight = self.token_emb.weight\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T = x.shape\n",
        "    pos = torch.arange(T).to(x.device)\n",
        "\n",
        "    x = self.token_emb(x) + self.pos_emb(pos)\n",
        "\n",
        "    for block in self.blockes:\n",
        "      x = block(x)\n",
        "\n",
        "    x = self.ln_f(x)\n",
        "    return self.head(x)\n"
      ],
      "metadata": {
        "id": "o2zop-GZm6iO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and Prepare Shakespeare Data"
      ],
      "metadata": {
        "id": "2ort13pJonhu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/tiny-shakespeare.txt\",\"r\", encoding=\"utf-8\") as f:\n",
        "  text = f.read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "stoi = {ch: i for i,ch in enumerate(chars)}\n",
        "itos = {i: ch for ch,i in stoi.items()}\n",
        "\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda x: ''.join([itos[i] for i in x])\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)"
      ],
      "metadata": {
        "id": "ibSLIDHSok6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch Sampling"
      ],
      "metadata": {
        "id": "uwm4y5WfqRe8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 64\n",
        "batch_size = 32\n",
        "\n",
        "def get_batch():\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  return x,y"
      ],
      "metadata": {
        "id": "oL9oVpnSp9T3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop"
      ],
      "metadata": {
        "id": "FxhpuOR-q_1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = TinyGPT(vocab_size, 128, 4, 512, 4, block_size).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "\n",
        "for step in range(2000):\n",
        "  x, y = get_batch()\n",
        "  x, y = x.to(device), y.to(device)\n",
        "\n",
        "  logits = model(x)\n",
        "  loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1))\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if step % 200 == 0:\n",
        "    print(f'step {step} | loss {loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iI5yS1Ixq5YC",
        "outputId": "e5bebe14-0838-46f6-92f5-1e3842788aec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0 | loss 81.7916\n",
            "step 200 | loss 4.0322\n",
            "step 400 | loss 3.4443\n",
            "step 600 | loss 3.1089\n",
            "step 800 | loss 2.9156\n",
            "step 1000 | loss 2.8356\n",
            "step 1200 | loss 2.7423\n",
            "step 1400 | loss 2.6017\n",
            "step 1600 | loss 2.5273\n",
            "step 1800 | loss 2.4535\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Generation"
      ],
      "metadata": {
        "id": "k36_dzYLsEaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, start, max_new_tokens=200):\n",
        "    model.eval()\n",
        "    idx = torch.tensor(encode(start), device=device)[None, :]\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -block_size:]   # ðŸ”‘ CRITICAL FIX\n",
        "\n",
        "        logits = model(idx_cond)\n",
        "        probs = F.softmax(logits[:, -1, :], dim=-1)\n",
        "        next_id = torch.multinomial(probs, 1)\n",
        "\n",
        "        idx = torch.cat([idx, next_id], dim=1)\n",
        "\n",
        "    return decode(idx[0].tolist())\n",
        "\n",
        "\n",
        "print(generate(model, \"ROMEO\"))\n"
      ],
      "metadata": {
        "id": "Hk-sfvLIsKYm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89a05613-7057-4d1f-d455-37f38322b304"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEOY: wir 'sic doy, whatusend woubupread fim to wars as od coce hen hes ist pa hy wes be berhat' ED:\n",
            "Thincame ad bl shithat farase, waits wiveastregn ss se thereatin\n",
            "Thow ates, and you wicerdiour ther ye\n"
          ]
        }
      ]
    }
  ]
}